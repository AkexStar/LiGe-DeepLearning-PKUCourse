{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "# 加载测试集\n",
    "test_data = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=1, shuffle=True)\n",
    "\n",
    "# 定义攻击函数\n",
    "def fgsm_attack(image, epsilon, data_grad):\n",
    "    # 计算扰动值\n",
    "    sign_data_grad = data_grad.sign()\n",
    "    perturbed_image = image + epsilon * sign_data_grad\n",
    "    # 限制像素值范围在[0,1]\n",
    "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "    return perturbed_image\n",
    "\n",
    "# 定义测试函数\n",
    "def test(model, device, test_loader, epsilon):\n",
    "    correct = 0\n",
    "    adv_examples = []\n",
    "    for data, target in test_loader:\n",
    "        # 找到被分类正确的图像\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        init_pred = output.max(1, keepdim=True)[1]\n",
    "        if init_pred.item() != target.item():\n",
    "            continue\n",
    "        correct += 1\n",
    "        # 对该图像进行攻击\n",
    "        data.requires_grad = True\n",
    "        output = model(data)\n",
    "        loss = torch.nn.functional.nll_loss(output, target)\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        data_grad = data.grad.data\n",
    "        perturbed_data = fgsm_attack(data, epsilon, data_grad)\n",
    "        # 将生成的对抗样本加入列表\n",
    "        adv_examples.append((data, perturbed_data, target))\n",
    "        # 打印进度信息\n",
    "        if len(adv_examples) >= 1000:\n",
    "            break\n",
    "        if len(adv_examples) % 100 == 0:\n",
    "            print(f\"Attack progress: {len(adv_examples)}/{1000}\")\n",
    "    # 计算攻击成功率\n",
    "    final_acc = correct / float(len(test_loader))\n",
    "    print(f\"Correctly classified examples: {correct}/{len(test_loader)}\")\n",
    "    print(f\"Attack success rate: {(1000-correct)/1000:.4f}\")\n",
    "    return adv_examples\n",
    "\n",
    "# 设置攻击参数\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "epsilon = 0.1\n",
    "\n",
    "# 对模型进行测试和攻击\n",
    "model.eval()\n",
    "adv_examples = test(model, device, test_loader, epsilon)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
